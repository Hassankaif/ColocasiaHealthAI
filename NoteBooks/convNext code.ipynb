{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install tensorflow==2.13.0\n",
    "!pip install tensorflow_addons shap  matplotlib seaborn pandas numpy scikit-learn\n",
    "#gradcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_addons'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_addons\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfa\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_addons'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "import os\n",
    "import shap\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# Update the dataset path for local system\n",
    "BASE_PATH = \"C:/Users/Hassan/Documents/Colocasia_Dataset/Dataset\"\n",
    "CLASS_NAMES = [\n",
    "    'Disease_Leaf_Blight_Dorsal',\n",
    "    'Disease_Leaf_Blight_Ventral',\n",
    "    'Disease_Mosaic_Dorsal',\n",
    "    'Disease_Mosaic_Ventral',\n",
    "    'Healthy_Dorsal',\n",
    "    'Healthy_Ventral'\n",
    "]\n",
    "\n",
    "CONFIG = {\n",
    "    'BATCH_SIZE': 32,\n",
    "    'IMAGE_SIZE': (224, 224),\n",
    "    'NUM_CLASSES': len(CLASS_NAMES),\n",
    "    'EPOCHS': 50,\n",
    "    'K_FOLDS': 6,\n",
    "    'SEED': 42,\n",
    "    'LEARNING_RATE': 1e-4\n",
    "}\n",
    "\n",
    "TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "CHECKPOINT_DIR = \"C:/Users/Hassan/Documents/colocasia_checkpoints\"\n",
    "RESULTS_DIR = \"C:/Users/Hassan/Documents/colocasia_results\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "def get_all_image_paths():\n",
    "    image_paths, labels = []\n",
    "    \n",
    "    for label, class_name in enumerate(CLASS_NAMES):\n",
    "        class_path = os.path.join(BASE_PATH, class_name)\n",
    "        for subdir in ['augmented_images', 'original_images']:\n",
    "            subdir_path = os.path.join(class_path, subdir)\n",
    "            if os.path.exists(subdir_path):\n",
    "                for filename in os.listdir(subdir_path):\n",
    "                    if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                        image_paths.append(os.path.join(subdir_path, filename))\n",
    "                        labels.append(label)\n",
    "    \n",
    "    return image_paths, labels\n",
    "\n",
    "def load_and_prepare_dataset():\n",
    "    image_paths, labels = get_all_image_paths()\n",
    "    if not image_paths:\n",
    "        print(\"‚ùå No images found! Check dataset structure.\")\n",
    "        return None, None\n",
    "\n",
    "    images = [tf.keras.preprocessing.image.img_to_array(\n",
    "        tf.keras.preprocessing.image.load_img(img_path, target_size=CONFIG['IMAGE_SIZE'])\n",
    "    ) / 255.0 for img_path in image_paths]\n",
    "    \n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "def create_convnext_model():\n",
    "    base_model = tf.keras.applications.ConvNeXtBase(\n",
    "        include_top=False, weights=\"imagenet\",\n",
    "        input_shape=(*CONFIG['IMAGE_SIZE'], 3), pooling='avg'\n",
    "    )\n",
    "    base_model.trainable = True\n",
    "    for layer in base_model.layers[:-30]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        base_model,\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(512, activation=\"swish\"),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        tf.keras.layers.Dense(CONFIG['NUM_CLASSES'], activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.AdamW(\n",
    "            learning_rate=CONFIG['LEARNING_RATE'], weight_decay=0.0001\n",
    "        ),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", tfa.metrics.F1Score(num_classes=CONFIG['NUM_CLASSES'], average=\"weighted\")]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate():\n",
    "    images, labels = load_and_prepare_dataset()\n",
    "    if images is None:\n",
    "        return\n",
    "\n",
    "    kf = KFold(n_splits=CONFIG['K_FOLDS'], shuffle=True, random_state=CONFIG['SEED'])\n",
    "    histories = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(images)):\n",
    "        print(f\"\\nüöÄ Training Fold {fold + 1}/{CONFIG['K_FOLDS']}\")\n",
    "        model_path = os.path.join(CHECKPOINT_DIR, f'model_fold_{fold+1}.h5')\n",
    "\n",
    "        model = create_convnext_model()\n",
    "        if os.path.exists(model_path):\n",
    "            print(f\"üîÑ Resuming training from {model_path}\")\n",
    "            model.load_weights(model_path)\n",
    "\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((images[train_idx], labels[train_idx]))\n",
    "        val_ds = tf.data.Dataset.from_tensor_slices((images[val_idx], labels[val_idx]))\n",
    "\n",
    "        train_ds = train_ds.batch(CONFIG['BATCH_SIZE'])\n",
    "        val_ds = val_ds.batch(CONFIG['BATCH_SIZE'])\n",
    "\n",
    "        callbacks = [\n",
    "            ModelCheckpoint(model_path, monitor='val_f1_score', save_best_only=True),\n",
    "            EarlyStopping(monitor='val_f1_score', patience=5, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(monitor='val_f1_score', factor=0.2, patience=3)\n",
    "        ]\n",
    "\n",
    "        history = model.fit(train_ds, validation_data=val_ds, epochs=CONFIG['EPOCHS'], callbacks=callbacks, verbose=1)\n",
    "        histories.append(history.history)\n",
    "        plot_fold_results(history, fold)\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "def plot_fold_results(history, fold):\n",
    "    metrics = ['accuracy', 'loss', 'f1_score']\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i, metric in enumerate(metrics):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        plt.plot(history.history[metric], label=f'Training {metric}')\n",
    "        plt.plot(history.history[f'val_{metric}'], label=f'Validation {metric}')\n",
    "        plt.title(f'Model {metric} - Fold {fold+1}')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(metric)\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, f'metrics_fold_{fold+1}.png'))\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üåø Starting Colocasia Plant Disease Classification Training\")\n",
    "    train_and_evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Dataset Configuration\n",
    "BASE_PATH = \"/content/drive/MyDrive/Colocasia Dataset/Dataset\"\n",
    "CLASS_NAMES = [\n",
    "    'Disease_Leaf_Blight_Dorsal',\n",
    "    'Disease_Leaf_Blight_Ventral',\n",
    "    'Disease_Mosaic_Dorsal',\n",
    "    'Disease_Mosaic_Ventral',\n",
    "    'Healthy_Dorsal',\n",
    "    'Healthy_Ventral'\n",
    "]\n",
    "\n",
    "CONFIG = {\n",
    "    'BATCH_SIZE': 32,\n",
    "    'IMAGE_SIZE': (224, 224),\n",
    "    'NUM_CLASSES': len(CLASS_NAMES),\n",
    "    'EPOCHS': 50,\n",
    "    'K_FOLDS': 6,\n",
    "    'SEED': 42,\n",
    "    'LEARNING_RATE': 1e-4\n",
    "}\n",
    "\n",
    "# Create timestamp for unique run identification\n",
    "TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Setup output directories\n",
    "CHECKPOINT_DIR = '/content/drive/MyDrive/colocasia_checkpoints'\n",
    "RESULTS_DIR = '/content/drive/MyDrive/colocasia_results'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "def get_all_image_paths():\n",
    "    \"\"\"Retrieve all image paths inside augmented_images and original_images folders.\"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "\n",
    "    for label, class_name in enumerate(CLASS_NAMES):\n",
    "        class_path = os.path.join(BASE_PATH, class_name)\n",
    "        for subdir in ['augmented_images', 'original_images']:\n",
    "            subdir_path = os.path.join(class_path, subdir)\n",
    "            if os.path.exists(subdir_path):\n",
    "                for filename in os.listdir(subdir_path):\n",
    "                    if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                        image_paths.append(os.path.join(subdir_path, filename))\n",
    "                        labels.append(label)\n",
    "\n",
    "    return image_paths, labels\n",
    "\n",
    "def load_and_prepare_dataset():\n",
    "    \"\"\"Load dataset paths and preprocess images.\"\"\"\n",
    "    print(\"\\nLoading dataset...\")\n",
    "\n",
    "    image_paths, labels = get_all_image_paths()\n",
    "    if not image_paths:\n",
    "        print(\"‚ùå No images found! Check dataset structure.\")\n",
    "        return None, None\n",
    "\n",
    "    # Load and preprocess images\n",
    "    images = []\n",
    "    for img_path in image_paths:\n",
    "        img = tf.keras.preprocessing.image.load_img(img_path, target_size=CONFIG['IMAGE_SIZE'])\n",
    "        img = tf.keras.preprocessing.image.img_to_array(img) / 255.0\n",
    "        images.append(img)\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "def create_convnext_model():\n",
    "    \"\"\"Create and compile ConvNeXt model.\"\"\"\n",
    "    base_model = tf.keras.applications.ConvNeXtBase(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        input_shape=(*CONFIG['IMAGE_SIZE'], 3),\n",
    "        pooling='avg'\n",
    "    )\n",
    "\n",
    "    base_model.trainable = True\n",
    "    for layer in base_model.layers[:-30]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        base_model,\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(512, activation=\"swish\"),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        tf.keras.layers.Dense(CONFIG['NUM_CLASSES'], activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.AdamW(\n",
    "            learning_rate=CONFIG['LEARNING_RATE'],\n",
    "            weight_decay=0.0001\n",
    "        ),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\n",
    "            \"accuracy\",\n",
    "            tfa.metrics.F1Score(num_classes=CONFIG['NUM_CLASSES'], average=\"weighted\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate():\n",
    "    \"\"\"Main training and evaluation function.\"\"\"\n",
    "    images, labels = load_and_prepare_dataset()\n",
    "    if images is None:\n",
    "        return\n",
    "\n",
    "    print(f\"\\n‚úì Loaded {len(images)} images\")\n",
    "    print(f\"‚úì Class distribution: {np.bincount(labels)}\")\n",
    "\n",
    "    kf = KFold(n_splits=CONFIG['K_FOLDS'], shuffle=True, random_state=CONFIG['SEED'])\n",
    "    histories = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(images)):\n",
    "        print(f\"\\nüöÄ Training Fold {fold + 1}/{CONFIG['K_FOLDS']}\")\n",
    "\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "            (images[train_idx], labels[train_idx])\n",
    "        ).batch(CONFIG['BATCH_SIZE'])\n",
    "\n",
    "        val_ds = tf.data.Dataset.from_tensor_slices(\n",
    "            (images[val_idx], labels[val_idx])\n",
    "        ).batch(CONFIG['BATCH_SIZE'])\n",
    "\n",
    "        model = create_convnext_model()\n",
    "\n",
    "        callbacks = [\n",
    "            ModelCheckpoint(\n",
    "                os.path.join(CHECKPOINT_DIR, f'model_fold_{fold+1}.h5'),\n",
    "                monitor='val_f1_score',\n",
    "                save_best_only=True\n",
    "            ),\n",
    "            EarlyStopping(monitor='val_f1_score', patience=5, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(monitor='val_f1_score', factor=0.2, patience=3)\n",
    "        ]\n",
    "\n",
    "        history = model.fit(\n",
    "            train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=CONFIG['EPOCHS'],\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        histories.append(history.history)\n",
    "        plot_fold_results(history, fold)\n",
    "\n",
    "        if fold == CONFIG['K_FOLDS'] - 1:\n",
    "            generate_shap_analysis(model, images[:100])\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "def plot_fold_results(history, fold):\n",
    "    \"\"\"Plot training results for each fold.\"\"\"\n",
    "    metrics = ['accuracy', 'loss', 'f1_score']\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        plt.plot(history.history[metric], label=f'Training {metric}')\n",
    "        plt.plot(history.history[f'val_{metric}'], label=f'Validation {metric}')\n",
    "        plt.title(f'Model {metric} - Fold {fold+1}')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(metric)\n",
    "        plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, f'metrics_fold_{fold+1}.png'))\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üåø Starting Colocasia Plant Disease Classification Training\")\n",
    "    train_and_evaluate()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
